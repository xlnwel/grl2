---
# NOTE: root_dir and model_name will be specified to all configs in run.train.py
algorithm: &algo sync2-zero
name: zero
version: 0

precision: 32

n_agents: 1

# model path: root_dir/model_name/name
# tensorboard path: root_dir/model_name/logs
# the following names are just examples; they will be re-specified in the entry point
root_dir: *algo
model_name: *algo

controller:
  store_period: 1e5
  restart_runners_period: null
  max_pbt_iterations: 1
  max_steps_per_iteration: 2e8
  initialize_rms: &irms True

parameter_server:
  root_dir: *algo
  model_name: *algo

  train_from_scratch_frac: 1
  online_frac: .2

  payoff:
    step_size: 1e-2   # step size towards the most recent data, 0 or null average payoff over the entire history
    update_interval: 180
    sampling_strategy:
      type: pfsp
      p: 1

  # rule_strategies:
  #   # agent name / config
  #   random:
  #     aid: 1
  #     vid: 1
  #     path: rule/random   # path to the file that defines Strategy
  #     # other configs

ray_config:
  runner:
    num_cpus: 1
  agent:
    num_gpus: 1

monitor: {}

runner:
  n_runners: &nrunners 2
  n_steps: &nsteps 4
  push_every_episode: False

env:
  env_name: &env_name grid_world-staghunt
  n_envs: &nenvs 16
  max_episode_steps: 50
  share_reward: False
  stag_stay_still: True
  reward_scale: 1
  population_size: 1
  representation: raw

  uid2aid: [0, 0]
  use_idx: True
  use_hidden: True
  use_event: False

agent: {}

strategy:
  algorithm: *algo
  train_loop: {}

model:
  aid: 0
  rnn_type: &rnn null
  rl_reward: &rlreward sum
  gamma: &gamma .99
  meta_reward_type: intrinsic
  K: &K 0
  L: &L 0
  extra_meta_step: &ems 1

  encoder: 
    nn_id: null
  rnn:
    nn_id: *rnn
    units: 64
    w_init: orthogonal
    recurrent_initializer: orthogonal
    use_ln: True
  policy:
    nn_id: policy
    units_list: [64, 64]
    w_init: orthogonal
    activation: relu
    norm: null
    out_scale: .01
    index: head
    index_config: 
      use_shared_bias: False
      use_bias: True
      w_init: orthogonal
  value:
    nn_id: value
    units_list: [64, 64]
    w_init: orthogonal
    activation: relu
    norm: null
    index: head
    index_config: 
      use_shared_bias: False
      use_bias: True
      w_init: orthogonal
  outer_value:
    nn_id: value
    units_list: [64, 64]
    w_init: orthogonal
    activation: relu
    norm: null
    index: head
    index_config:
      use_shared_bias: False
      use_bias: True
      w_init: orthogonal
  meta_reward: 
    nn_id: reward
    units_list: [64, 64]
    w_init: orthogonal
    activation: relu
    norm: null
    out_scale: .01
    out_act: atan
    combine_xa: False
    index: head
    index_config:
      use_shared_bias: False
      use_bias: True
      w_init: orthogonal
  meta_params:
    nn_id: meta
    pg_coef:
      outer: 1
      default: 1
      init: null
      act: sigmoid
    entropy_coef:
      outer: 1e-2
      default: 1e-2
      scale: 1e-2
      bias: 0
      init: null
      act: sigmoid
    value_coef:
      outer: .5
      default: .5
      scale: 1
      bias: 0
      init: null
      act: sigmoid
    gamma: 
      outer: *gamma
      default: *gamma
      scale: 1
      bias: 0
      init: null
      act: sigmoid
    lam:
      outer: .95
      default: .95
      scale: 1
      bias: 0
      init: null
      act: sigmoid
    reward_scale:
      outer: 1
      default: 1
      scale: 1
      bias: 0
      init: null
      act: sigmoid
    reward_bias:
      outer: 0
      default: 0
      scale: 1
      bias: 0
      init: null
      act: null
    reward_coef:
      outer: 1
      default: 1
      scale: 1
      bias: 0
      init: null
      act: tanh

loss:
  # hyperparams for value target and advantage
  target_type: vtrace
  c_clip: 1
  rho_clip: 1
  adv_type: vtrace
  norm_adv: False
  norm_meta_adv: False

  # hyperparams for policy optimization
  pg_type: pg 
  ppo_clip_range: .2
  use_dice: True
  dice_axis: null
  dice_lam: 1
  kl: reverse
  kl_coef: 1
  policy_sample_mask: True

  # hyperparams for value learning
  value_loss: mse
  value_clip_range: .2
  value_sample_mask: False
  stop_target_grads: False

  # hyperparams for meta-learning
  meta_reward_coef: 0
  joint_objective: False

trainer:
  algorithm: *algo
  aid: 0
  n_runners: *nrunners
  n_envs: *nenvs
  n_epochs: &nepochs 1
  n_mbs: &nmbs 1
  n_meta_epochs: 1
  n_steps: *nsteps     # BPTT length
  timeout_done: &td True
  meta_type: plain
  K: *K
  L: *L
  msmg_type: avg
  extra_meta_step: *ems
  store_state: True
  event_done: False

  theta_opt:
    opt_name: rmsprop
    lr: 3e-3
    clip_norm: 10
    eps: 1e-5
  phi_opt:
    opt_name: rmsprop
    lr: 3e-3
    clip_norm: 10
    eps: 1e-5
  meta_params_opt:
    opt_name: rmsprop
    lr: 1e-4
    clip_norm: 1
    eps: 1e-5
  meta_reward_opt:
    opt_name: rmsprop
    lr: 1e-4
    clip_norm: 1
    eps: 1e-5

actor:
  algorithm: *algo

  rms:
    obs_names: [obs, hidden_state]
    normalize_obs: *irms
    normalize_reward: False
    obs_normalized_axis: [0, 1]
    reward_normalized_axis: [0, 1]
    update_reward_rms_in_time: True
    gamma: *gamma

buffer:
  type: ac

  n_runners: *nrunners
  n_envs: *nenvs
  n_steps: *nsteps
  queue_size: 2
  K: *K
  L: *L
  extra_meta_step: *ems
  timeout_done: *td
  rnn_type: *rnn

  # mini-batch size = n_runners * n_envs * epslen / n_mbs
  sample_keys:
    - obs
    - action
    - value
    - reward
    - discount
    - reset
    - mu_logprob
    - mu
    - mask
    - h
    - c
